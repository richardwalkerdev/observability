== Prometheus

Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License.

The good news is that Prometheus is at the heart of this whole micro-service architecture. At its most basic it could be all that is needed. Any target to be scraped for metrics and any alerting rule is all done here using Prometheus. Every other component is peripheral, either extending or handing off responsibility or consuming data for visualisation and storage.

This section deal with deploying Prometheus on two Virtual Machines, mounting NFS shares for the target and rules configuration and load balancing the two Prometheus instances. Think of each instance of Prometheus as a Replica.

image::images/prometheus.png[Prometheus]

Add a service user account:

[source%nowrap,bash]
----
useradd -m -s /bin/false prometheus
----

Create two directories:

[source%nowrap,bash]
----
mkdir -p /etc/prometheus /var/lib/prometheus
----

Change ownership of directories:

[source%nowrap,bash]
----
chown prometheus:prometheus /etc/prometheus /var/lib/prometheus/
----

Get the latest download link from https://prometheus.io/download/:

[source%nowrap,bash]
----
dnf install wget -y
wget https://github.com/prometheus/prometheus/releases/download/v2.24.1/prometheus-2.24.1.linux-amd64.tar.gz
----

Extract the archive and copy binaries into place:

[source%nowrap,bash]
----
dnf install tar -y
tar -xvf prometheus-2.24.1.linux-amd64.tar.gz
cd prometheus-2.24.1.linux-amd64
cp prometheus promtool /usr/local/bin/
----

Check the path is correct and versions:

[source%nowrap,bash]
----
prometheus --version
promtool --version
----

Always use the IP Address or preferably DNS name, not `localhost` for scrape targets:

IMPORTANT: Global `external_labels` are added to either identify each prometheus instances in a HA configuration or the prometheus cluster, if labels are identical on each instance.

*Same config on both nodes*

[source%nowrap,bash]
----
vi /etc/prometheus/prometheus.yml
----

[source%nowrap,yaml]
----
# Global config
global:
  scrape_interval:     15s
  evaluation_interval: 15s
  scrape_timeout: 15s
  external_labels:
    cluster: prometheus-cluster
    region: europe
    environment: dev

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
    - targets: ['0.0.0.0:9090']
----

Note that the `scrape_configs` includes only this prometheus target at this stage. In other words a running instance of Prometheus exposes metrics about itself, when further instances are added they need to be included for example:

[source%nowrap,yaml]
----
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
    - targets: ['0.0.0.0:9090','192.168.0.72']
----

Create a service using `systemd`, adding `--web.listen-address=:9090`:

To reduce the time to begin archiving use minutes, example:

[source%nowrap]
----
    --storage.tsdb.max-block-duration=30m \
    --storage.tsdb.min-block-duration=30m \
----

[source%nowrap,bash]
----
vi /etc/systemd/system/prometheus.service
----

[source%nowrap,bash]
----
[Unit]
Description=Prometheus Service
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
LimitNOFILE=65536
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --storage.tsdb.max-block-duration=2h \
    --storage.tsdb.min-block-duration=2h \
    --web.listen-address=:9090

[Install]
WantedBy=multi-user.target
----

Start and enable the Prometheus:

[source%nowrap,bash]
----
systemctl daemon-reload
systemctl enable prometheus --now
systemctl status prometheus
----

NOTE: Prometheus store its data under `/var/lib/prometheus` by default.

Open firewall port:

[source%nowrap,bash]
----
firewall-cmd --add-port=9090/tcp --permanent
firewall-cmd --reload
----

A single Prometheus instance can then be access using a browser for example: http://192.168.0.71:9090/. Assuming all these steps have been repeated on a second node (192.168.0.72), add a load balancer for these two Prometheus instances using HAProxy.

On the host serving HAProxy:

[source%nowrap,bash]
----
vi /etc/haproxy/haproxy.cfg
----

[source%nowrap,bash]
----
# Prometheus LB
frontend prometheus-lb-frontend
    bind 192.168.0.70:9090
    default_backend prometheus-lb-backend

backend prometheus-lb-backend
    balance roundrobin
    server prometheus1 192.168.0.71:9090 check
    server prometheus2 192.168.0.72:9090 check
----

And restart HAProxy plus checking the status:

[source%nowrap,bash]
----
systemctl restart haproxy
systemctl status haproxy
----

Open firewall on HAProxy host too:

[source%nowrap,bash]
----
firewall-cmd --add-port=9090/tcp --permanent
firewall-cmd --reload
----

View the state of the load balancer using a browser at http://192.168.0.70:9000/stats.

View Prometheus via the load balancer using http://192.168.0.70:9090/.

=== Basics

A prometheus instance exposes metrics about itself, for example http://192.168.0.71:9090/metrics and the only target configuration included (at this stage) is itself.

Look at Targets in a browser:

image::images/prom-1.png[Prometheus]

Execute a query:

[source%nowrap,bash]
----
promhttp_metric_handler_requests_total{code="200"}
----

image::images/prom-3.png[Prometheus]

And observe there are no alerts configured yet:

image::images/prom-2.png[Prometheus]

=== Decouple config

Remember to think of each instance of Prometheus as a Replica behind the load balancer, this mean any instance of Prometheus need the same configuration. Deploying this stack natively on VMs or cloud instances (oppose to using containers), the config directories might as well be mounted file systems.

Make two directories for the target config and rules:

[source%nowrap,bash]
----
mkdir -p /etc/prometheus/targets /etc/prometheus/rules
----

Added the following to `fstab`:

[source%nowrap,bash]
----
vi /etc/fstab
----

[source%nowrap,bash]
----
192.168.0.70:/nfs/targets /etc/prometheus/targets nfs rw,sync,hard,intr 0 0
192.168.0.70:/nfs/rules /etc/prometheus/rules nfs rw,sync,hard,intr 0 0
----

Ensure `nfs-utils` is installed:

[source%nowrap,bash]
----
dnf install nfs-utils -y
----

And mount the NFS shares (created at the start of this page):

[source%nowrap,bash]
----
mount -a
----

Now update the Prometheus configuration to read files from those directories for both `tartgets` and `rules`:

[source%nowrap,bash]
----
vi /etc/prometheus/prometheus.yml
----

[source%nowrap,yaml]
----
scrape_configs:
  - job_name: 'targets'
    file_sd_configs:
    - files:
      - /etc/prometheus/targets/*.yml

rule_files:
  - /etc/prometheus/rules/*.yml
----

And add the Prometheus target/s:

[source%nowrap,bash]
----
vi /etc/prometheus/targets/prometheus_targets.yml
----

[source%nowrap,yaml]
----
---
- labels:
    service: prometheus
    env: staging
  targets:
  - 192.168.0.71:9090
----

Restart Prometheus:

[source%nowrap,yaml]
----
systemctl restart prometheus
----

Everything should be the same except now the configuration is decoupled from any instance of Prometheus. When the second instance is added in this example `prometheus_targets.yml` should include both instances:

[source%nowrap,yaml]
----
---
- labels:
    service: prometheus
    env: staging
  targets:
  - 192.168.0.71:9090
  - 192.168.0.72:9090
----

=== Chronyd

It's a good idea to make sure all servers and clients are in sync with their clocks, for reference:

[source%nowrap,yaml]
----
dnf install chrony
systemctl start chronyd
systemctl enable chronyd
chronyc tracking
----

=== Recap

Everything from this point on involves adding target scrape configurations and rules, specifically alert rules for Prometheus. All the other components are *peripheral* to Prometheus, either extending or handing off services, or consuming data for other purposes, as in the case of Grafana that using Prometheus as a data source for displaying information in a graphical way.

// This is a comment and won't be rendered.
